---
title: "Polling"
author: "R package build"
date: "2022-09-23"
slug: []
categories: []
summary: In this post, I look at the relationship between polling and US house voting
  patterns.
tags: []
---

# Intro

In this blog post, I  choose to explore both Extension #1 and
Extension #2. For Extension #1, I will compare forecasting methods from The Economist
and FiveThirtyEight, as well as giving my own two cents on the approach. In extension #2,
I will look at historical generic ballot data, build my own model,
and incorporate nation-wide level economic data to build a nationwide 
vote share model. I will not include a part for the Ohio-01
race because there has only been one poll so far.

# Extension 1: Comparing FiveThirtyEight to the Economist

In FiveThirtyEight's methodology report about their 2022 2022 midterm forecasts, 
they detail their three main models: Lite, Classic, and Deluxe. Classic, their
main model, incorporates polling data, CANTOR (a system that uses inference to 
get comparable polling stats from districts without polls), and Fundamentals(variables 
like fundraising, approval ratings, incumbency status, former voting patterns).
Their Lite model only uses Polls and CANTOR-based polling inference,
while their Deluxe model is the Classic along with expert forecasts ratings
for races.

FiveThirtyEigjhty does not simply treat all polls as equal - they have a rigorous
process for rating polls based on how their methods, past performance, and company
standards. Then, FiveThirtyEight weights polls in their models based off their rating.

In terms of modeling uncertainty, the FiveThirty Eight model includes for the possibility
of a 'uniform national swing,' or situation where all the polls were biased in one
party's direction. Similarly, their model projects turnout and includes these variances
into their overall distribution. While the article didn't go into specific detail
on their simulation processes, it is important to note that their CANTOR machine
simulates a grid of possible variables to model a variety of outcomes.

The Economist uses many similar techniques to FiveThirtyEight, but appears to rely
significantly less on local ballots and more on the generic ballot. 

Their approach begins at the nationwide-level, using the generic ballot results to detect the pulse of
each party's popularity. They also incorporate presidential approval, polarisation, and partisan lean
into their voting-based predictors. Finally, this first stage model includes a series of 
fundamentals, defined in the article as: type of election year, presidential re-election status, 
and unemployment.

Next, The Economist model begins incorporating district-level data, which includes 
voting history and candidacy characteristics. In order to model the turnout, they 
look at nationwide swing voters and popular vote. In the article, they are quick
to mention that point estimates are very challenging at the house level because they
don't follow a normal distribution. Hence, they use a "skew-T" model to allow for the long tails.
Similar to FiveThirtyEight, the Economist undergoes thousands of simulations and
uses their nation-wide level data to inform their distributions for each individual house race.


I believe the Economist's method is more precise because it is less reliant
on local polling, which is subject to high variation and bias. I imagine that 
the Economist weights their polls by their internal ratings of the pollsters, but
it was unclear from the article if they had as precise a method as FiveThirtyEight.


# Extension #2/Model Improvements

Examining variation in pollster quality

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(lubridate)
library(stargazer)
library("rmarkdown")
library("tinytex")
library("stargazer")
library("sandwich")
library('patchwork')
library('jtools')
library('huxtable')
library(usmap)
library(geomtextpath)
library(caret)

gen_poll<-read.csv("data/GenericPolls1942_2020.csv")

inc_pop_vote<-read.csv("data/inc_pop_vote_df.csv")%>%
  filter(party==winner_party.x)

poll_18<-read.csv("data/538_generic_poll_2018.csv")
poll_22<-read.csv("data/generic_ballot_polls_22.csv")
poll_order<-c('A+','A','A-','A/B','B+','B','B-','B/C','C','C/D','F')

```

```{r, poll ratings, message=FALSE,warning=FALSE}
var_18<-ggplot(data=poll_18,aes(x=factor(fte_grade,levels=poll_order)))+
  geom_histogram(stat='count',col='green')+
  labs(title='2018 House-Specific polls',x='poll rating')
var_22<-ggplot(data=poll_22,aes(x=factor(fte_grade,levels=poll_order)))+geom_histogram(stat='count',col='pink')+labs(title='2022 Generic Ballot Polls',x='poll rating')
(var_18|var_22)+plot_annotation(title='Variance in 538 Pollster Grades by Election Year and Type of Poll')
```

As these graphs show, 538 has higher ratings, on average, for  their 2018 district-level polls  than their generic polls in 2022. In both graphs, there is significant variation
in poll ratings, ranging from about A+ to C/D or F.


Next, I do some preliminary model building to see how well generic ballots performed
using historical data dating back to 1946. 

```{r,message=FALSE,include=FALSE}

join<-left_join(gen_poll,inc_pop_vote,by='year')%>%
  mutate(R_vote=case_when(winner_party.x=='R'~majorvote_pct,
                          TRUE~100-majorvote_pct),
         Rep_2=rep/(dem+rep)*100,
         Dem_2=100-Rep_2,
         ElectionYear=ifelse(year%%2,0,1),
         MidtermYear=ifelse(ElectionYear==1&year%%4,1,0))
gdp<-read.csv('data/Copy of GDP_quarterly.csv')%>%
  mutate(ElectionYear=ifelse(year%%2,0,1))%>%
  filter(ElectionYear==1&quarter_cycle==7)
join<-left_join(join,gdp,by='year')

ctrl <- trainControl(method = "LOOCV")

#fit a regression model and use LOOCV to evaluate performance
data=join%>%filter(year>1946)
model <- train(R_vote~MidtermYear*president_party +
    log(days_until_election)+Rep_2, data=data,method = "lm", trControl = ctrl,
    na.action = na.exclude)
model2<- train(R_vote~MidtermYear*president_party +
    log(days_until_election)+Rep_2+GDP_growth_pct, data=data,method = "lm", trControl = ctrl,
    na.action = na.exclude)

#view summary of LOOCV               
print(model)
print(model2)
model2<- lm(R_vote~MidtermYear*president_party +
    log(days_until_election)+Rep_2+GDP_growth_pct, data=data)

lm1<-lm(R_vote~MidtermYear*president_party +
    log(days_until_election)*Rep_2+GDP_growth_pct,data=join)
poll_22<-read.csv("data/538_generic_poll_2022.csv")
poll_22<-poll_22 %>%
  mutate('Rep_2'=adjusted_rep/(adjusted_rep+adjusted_dem)*100)%>%
  mutate(poll_date=mdy(enddate))
poll_22$president_party<-'D'
poll_22$MidtermYear<-1
poll_22$GDP_growth_pct<-1

electionday<-as.Date('2022-11-09')
poll_22$days_until_election<-as.numeric(difftime(electionday,poll_22$poll_date, units = "days"))
poll_22$pred<-predict(model2,poll_22)
join$pred<-predict(lm1,join)


```


```{r,results='asis'}

stargazer(model2,type='latex')
```


```{r predictions,message=FALSE,warning=FALSE}

ggplot(data=join%>% filter(MidtermYear==1&days_until_election<200&days_until_election>0))+
  geom_smooth(aes(y=Rep_2,x=days_until_election),col='red')+
  geom_smooth(aes(y=Dem_2,x=days_until_election),col='blue')+
  geom_point(aes(x=0,y=majorvote_pct,col=winner_party.x))+
   scale_color_manual(values=c("Blue",'Red'),labels=c('Dem','Rep'))+
  scale_x_reverse()+facet_wrap(~year)+labs(col='Winner',y='Vote share',
   title='Generic Ballot Vote Share by year and days until election',
   subtitle='Lines represent generic ballot, Point at day 0 represent Actual results of Election')+
  ylim(40,60)

ggplot(data=join%>% filter(MidtermYear==1&ElectionYear.x==1&days_until_election<200&days_until_election>0))+
  geom_smooth(aes(y=pred,x=days_until_election),col='red')+
    geom_smooth(aes(y=(100-pred),x=days_until_election),col='blue')+
  geom_point(aes(x=0,y=majorvote_pct,col=winner_party.x))+
   scale_color_manual(values=c("Blue",'Red'),labels=c('Dem','Rep'))+
  scale_x_reverse()+facet_wrap(~year)+labs(col='Winner',y='Vote share',
   title='Generic Ballot Vote Share by year and days until election',
   subtitle='Lines represent Predicted vote share, Point at day 0 represent Actual results of Election')+ylim(40,60)

```

These graphs demonstrate that as an polls gets closer to the election date,
the prediction error decreases. I have not yet weighted by the pollster rating
because it was challenging to join properly, but intend to work on that in upcoming weeks.
Similarly, I will think about whether or not I would like to do my model at the nationwide
or state level, but wanted to work on building a national level model since the last
blog used a state-by-state prediction

```{r,message=FALSE,warning=FALSE}


ggplot(data=poll_22%>% filter(days_until_election<200&days_until_election>0))+
  geom_smooth(aes(y=Rep_2,x=days_until_election),col='red')+
  geom_smooth(aes(y=pred,x=days_until_election),linetype='dashed',col='black')+
  scale_x_reverse()+labs(col='Winner',y='Vote share',
   title='Republican Vote Share by days until election',
   subtitle='Dashed line represents model predictions, red line represents averages by generic ballot day')

```

