---
title: "Polling"
author: "R package build"
date: "2022-09-23"
slug: []
categories: []
summary: In this post, I look at the relationship between polling and US house voting
  patterns. I begin with an analysis of midterm forecasting methods from 
  FiveThirtyEight and The Economist and end with a new model for predicting 
  popular vote house share in 2022.
tags: []
---

# Intro

In this blog post, I  choose to explore both Extension #1 and
Extension #2. For Extension #1, I will compare forecasting methods from The Economist
and FiveThirtyEight, as well as giving my own two cents on the approach. In extension #2,
I will look at historical generic ballot data, build my own model,
and incorporate nation-wide level economic data to build a nationwide 
vote share model. I will not include a part for the Ohio-01
race because there has only been one poll so far.

# Extension 1: Comparing FiveThirtyEight to the Economist

**538 Model 

In FiveThirtyEight's methodology report about their 2022 2022 midterm forecasts, 
they detail their three main models: Lite, Classic, and Deluxe. Classic, their
main model, incorporates polling data, CANTOR (a system that uses inference to 
get comparable polling stats from districts without polls), and Fundamentals(variables 
like fundraising, approval ratings, incumbency status, former voting patterns).
Their Lite model only uses Polls and CANTOR-based polling inference,
while their Deluxe model is the Classic along with expert forecasts ratings
for races.

FiveThirtyEigjhty does not simply treat all polls as equal - they have a rigorous
process for rating polls based on how their methods, past performance, and company
standards. Then, FiveThirtyEight weights polls in their models based off their rating.

In terms of modeling uncertainty, the FiveThirty Eight model includes for the possibility
of a 'uniform national swing,' or situation where all the polls were biased in one
party's direction. Similarly, their model projects turnout and includes these variances
into their overall distribution. While the article didn't go into specific detail
on their simulation processes, it is important to note that their CANTOR machine
simulates a grid of possible variables to model a variety of outcomes.

* Economist Model
The Economist uses many similar techniques to FiveThirtyEight, but appears to rely
significantly less on local ballots and more on the generic ballot. 

Their approach begins at the nationwide-level, using the generic ballot results to detect the pulse of
each party's popularity. They also incorporate presidential approval, polarisation, and partisan lean
into their voting-based predictors. Finally, this first stage model includes a series of 
fundamentals, defined in the article as: type of election year, presidential re-election status, 
and unemployment.

Next, The Economist model begins incorporating district-level data, which includes 
voting history and candidacy characteristics. In order to model the turnout, they 
look at nationwide swing voters and popular vote. In the article, they are quick
to mention that point estimates are very challenging at the house level because they
don't follow a normal distribution. Hence, they use a "skew-T" model to allow for the long tails.
Similar to FiveThirtyEight, the Economist undergoes thousands of simulations and
uses their nation-wide level data to inform their distributions for each individual house race.

*Overall Thoughts

I believe the Economist's method is more precise because it is less reliant
on local polling, which is subject to high variation and bias. I imagine that 
the Economist weights their polls by their internal ratings of the pollsters, but
it was unclear from the article if they had as precise a method as FiveThirtyEight.


# Extension #2/Model Improvements

Examining variation in pollster quality

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(lubridate)
library(stargazer)
library("rmarkdown")
library("tinytex")
library("stargazer")
library("sandwich")
library('patchwork')
library('jtools')
library('huxtable')
library(usmap)
library(geomtextpath)
library(caret)

gen_poll<-read.csv("data/GenericPolls1942_2020.csv")

inc_pop_vote<-read.csv("data/inc_pop_vote_df.csv")%>%
  filter(party==winner_party.x)

poll_18<-read.csv("data/538_generic_poll_2018.csv")
poll_22<-read.csv("data/generic_ballot_polls_22.csv")
poll_order<-c('A+','A','A-','A/B','B+','B','B-','B/C','C','C/D','F')

```

```{r, poll ratings, message=FALSE,warning=FALSE}
var_18<-ggplot(data=poll_18,aes(x=factor(fte_grade,levels=poll_order)))+
  geom_histogram(stat='count',col='green')+
  labs(title='2018 House-Specific polls',x='poll rating')
var_22<-ggplot(data=poll_22,aes(x=factor(fte_grade,levels=poll_order)))+geom_histogram(stat='count',col='pink')+labs(title='2022 Generic Ballot Polls',x='poll rating')
(var_18|var_22)+plot_annotation(title='Variance in 538 Pollster Grades by Election Year and Type of Poll')
```

As these graphs show, 538 has higher ratings, on average, for  their 2018 district-level polls  than their generic polls in 2022. In both graphs, there is significant variation
in poll ratings, ranging from about A+ to C/D or F.


Next, I do some preliminary model building to see how well generic ballots performed
using historical data dating back to 1946. 

```{r,message=FALSE,include=FALSE}
# Join generic ballot polls to incumbent popular vote dataset
join<-left_join(gen_poll,inc_pop_vote,by='year')%>%
  mutate(R_vote=case_when(winner_party.x=='R'~majorvote_pct,
                          TRUE~100-majorvote_pct),
         Rep_2=rep/(dem+rep)*100,
         Dem_2=100-Rep_2,
         ElectionYear=ifelse(year%%2,0,1),
         MidtermYear=ifelse(ElectionYear==1&year%%4,1,0))
gdp<-read.csv('data/Copy of GDP_quarterly.csv')%>%
  mutate(ElectionYear=ifelse(year%%2,0,1))%>%
  filter(ElectionYear==1&quarter_cycle==7)

# Join to gdp data
join<-left_join(join,gdp,by='year')

ctrl <- trainControl(method = "LOOCV")

#fit a regression model and use LOOCV to evaluate performance
data=join%>%filter(year>1946)
model <- train(R_vote~MidtermYear*president_party +
    log(days_until_election)+Rep_2, data=data,method = "lm", trControl = ctrl,
    na.action = na.exclude)
model2<- train(R_vote~MidtermYear*president_party +
    log(days_until_election)+Rep_2+GDP_growth_pct, data=data,method = "lm", trControl = ctrl,
    na.action = na.exclude)

#view summary of LOOCV               
print(model)
print(model2)

model2<- lm(R_vote~MidtermYear*president_party +
    log(days_until_election)+Rep_2+GDP_growth_pct, data=data)

# Building 2022 predictions dataset

poll_22<-read.csv("data/538_generic_poll_2022.csv")
poll_22<-poll_22 %>%
  mutate('Rep_2'=adjusted_rep/(adjusted_rep+adjusted_dem)*100)%>%
  mutate(poll_date=mdy(enddate))
poll_22$president_party<-'D'
poll_22$MidtermYear<-1
poll_22$GDP_growth_pct<-1
electionday<-as.Date('2022-11-09')
poll_22$days_until_election<-as.numeric(difftime(electionday,poll_22$poll_date, units = "days"))
poll_22$pred<-predict(model2,poll_22)

#adding historical predictions to dataset
join$pred<-predict(model2,join)


```


```{r,results='asis'}
summary(model2)

```

Surprisingly, the variable for logged days until election does not appear as significant
in my regression output. Instead, the model detects that the Republican share (denoted as Rep_2),
year status, presidential party, and gdp growth are most important. I have not yet weighted by the pollster rating
because it was challenging to join properly, but intend to work on that in upcoming weeks.
Similarly, I will think about whether or not I would like to do my model at the nationwide
or state level, but wanted to work on building a national level model since the last
blog used a state-by-state prediction


```{r predictions,message=FALSE,warning=FALSE}
gen_means<-join %>% filter(days_until_election<20)%>%
  group_by(year)%>%
  summarise(mean_gen_ballot=mean(Rep_2,na.rm=T),
            R_vote=mean(R_vote),
            pred=mean(pred,na.rm=T))
rmse_gen<-sqrt(mean(gen_means$mean_gen_ballot-gen_means$R_vote,na.rm=T)^2)
rmse_pred<-sqrt(mean(gen_means$pred-gen_means$R_vote,na.rm=T)^2)

ggplot(data=gen_means)+geom_smooth(aes(x=year,y=mean_gen_ballot,col='Generic Ballot Avg \n in 20 days before election'))+
  geom_smooth(aes(x=year,y=R_vote,col='Actual Share'))+
  geom_smooth(aes(x=year,y=pred,col='Predicted Share'))+
  labs(y='Republican Vote Share',col='Calculated Through:')

```

These graphs demonstrate that my prediction was, on average, closer to the 
actual results than simply looking at the average generic ballot, particularly in 
years preceding 1980. In recent years, however, the lines appear very close,
suggesting that there is  room for improvement in this nationwide model
if I want it to be more predictive than the simple generic ballot average.

Finally, I built a predicted Republican vote share for 2022, looking at
my predictions and the generic ballot averages vs. days before election.

```{r,message=FALSE,warning=FALSE}


ggplot(data=poll_22%>% filter(days_until_election<200&days_until_election>0))+
  geom_smooth(aes(y=Rep_2,x=days_until_election),col='red')+
  geom_smooth(aes(y=pred,x=days_until_election),linetype='dashed',col='black')+
  scale_x_reverse()+labs(col='Winner',y='Vote share',
   title='Republican Vote Share by days until election',
   subtitle='Dashed line represents model predictions, red line represents averages by generic ballot day')

```

#Conclusion

In this post, I explored the differences in Congressional modeling between
two of the leading forecasting publications: FiveThirtyEight and the Economist.
Additionally, I worked on building a nationwide vote share model using polling,
economic, and incumbency data. In the future weeks, I look forward to finding ways
to properly weight generic ballot polling and include it in my state-level model.

#Appendix

```{r}
# Generic Ballot vs actual results
ggplot(data=join%>% filter(MidtermYear==1&days_until_election<200&days_until_election>0))+
  geom_smooth(aes(y=Rep_2,x=days_until_election),col='red')+
  geom_smooth(aes(y=Dem_2,x=days_until_election),col='blue')+
  geom_point(aes(x=0,y=majorvote_pct,col=winner_party.x))+
   scale_color_manual(values=c("Blue",'Red'),labels=c('Dem','Rep'))+
  scale_x_reverse()+facet_wrap(~year)+labs(col='Winner',y='Vote share',
   title='Generic Ballot Vote Share by year and days until election',
   subtitle='Lines represent generic ballot, Point at day 0 represent Actual results of Election')+
  ylim(40,60)

#predictions vs actual results
ggplot(data=join%>% filter(MidtermYear==1&ElectionYear.x==1&days_until_election<200&days_until_election>0))+
         geom_smooth(aes(y=pred,x=days_until_election),col='red')+
    geom_smooth(aes(y=(100-pred),x=days_until_election),col='blue')+
  geom_point(aes(x=0,y=majorvote_pct,col=winner_party.x))+
   scale_color_manual(values=c("Blue",'Red'),labels=c('Dem','Rep'))+
  scale_x_reverse()+facet_wrap(~year)+labs(col='Winner',y='Vote share',
   title='Predicted Vote Share by year and days until election',
   subtitle='Lines represent Predicted vote share, Point at day 0 represent Actual results of Election')+ylim(40,60)
```

